---
title: "Model and Data Exploration Standards in Python with Markdown"
author: "Dave Campbell"
date: "11/01/2024"
output: html_document
---




```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



This document is meant to be part of a set of templates that showcase good coding and modern reproducibility practices, here using Python and Markdown.   

# Goal:
This document is a template for using Python with Markdown and shows how to download and prepare StatCan data from CanSim tables.  Data is downloaded from the source, directly pulling data from its url.  For transparency and robustness to future dataset changes, variable names are cleaned so that columns can be called by name rather than location.  Values and column names are recoded into longer, more meaningful values.  This will ease the overhead of sharing data, code, and results because the data can be examined, manipulated, and filtered without constant reference to the metadata coding.  

# Reproducible, transparent analysis

Guidelines are independent of the software used but may vary depending on our collaborators / use case.  This document will focus on using [Markdown](https://www.markdownguide.org/cheat-sheet) with Python, but the principles are universal across other languages and [Jupyter](http://datacamp-community-prod.s3.amazonaws.com/21fdc814-3f08-4aa9-90fa-247eedefd655) Notebooks.

 Markdown (and Jupyter) is designed to work with different software engines.  Regardless of whether you are using R or Python you can compile a Markdown document using the **knit** button (if using RStudio).  Using Python the **markdown** librart can be used to convert a document into html or pdf. Software languages are defined in the code chunk.    Note that these are run independently and their environments are not shared even when using both languages in one Markdown document.  



# Conventions for Writing Sharable Collaborative Code in Markdown or Notebooks.

- Using a mix of comments, code and output will make it easier to follow, share with collaborators,  reproduce, and alter.

- Make sure that variable names are well defined.  It's better for the sake of transferability between R and Python to use '\_' as a separator for long variable names although if using R '.' is also ok.

- Keep your object and function names descriptive and make sure your code names are consistent with the comment descriptions.

- It's often infeasible to show all the variables from a big data table, but it is still useful to show a few data rows and columns.

- Align your code vertically so that differences between similar lines stand out.

- Document your data for reproducibility and trustworthy analysis.

- Automate as much as possible to avoid errors and to ensure transparency.  This will also simplify making changes in future.

- Consider making single purpose Markdown file and clarify the goal at the top of the document.  

- Save output (data?) at the end of the document and name the file similar to the name of the Markdown document so that the creation and result are clearly linked.

- It's generally better practice to spit large projects into bite sized pieces.  In some cases that will be functions, in other cases it will be Markdown documents with a single focus such as loading, exploring, or analyzing data.


There are many other tricks that we'll use along the way depending on the goal of the code and audience but everything from here up sets a minimum standard for transparency and reproducibility.



# Going Futher and an Example



## Setting up Python in Markdown

Some preamble first. Using RStudio and Markdown, start by defining which python engine to use.  I ran  **which python** in the terminal to get the directory location for python.  All code chunks after this are run using this python engine.  Using the **reticulate** library, you can get fancy with this set up and pass objects back and forth between python and R.  RStudio can also run python directly by setting up **File**-> **New File** -> **Python Script**.


```{r}
# this chunk is using R, but all the rest will be using python.
# 
# library that will send set up the python engine in Markdown / RStudio IDE
library(reticulate)

use_python('/Users/iamdavecampbell/anaconda3/bin/python')  # Tell R which python environment to use. 
# alternatively this could
```

## Setting up the environment
For transparency, print the version for the software at the top of the document. 


```{python   pythonversion}
#using python:
import sys
print (sys.version)
```


Load libraries at the top of the document.  It helps the user to make sure that all libraries are installed and will help us to make sure the code will run on a different machine. It's nice to say something about the library.

```{python   library, warning = FALSE, message = FALSE}
import requests          # for downloading
import zipfile           # for unzipping data
from io import BytesIO   # for file handling
import pandas as pd      # for data handling
import re                # for string detection
```



### Code chunks
 The general format for code chunks is to specify the code engine (here, R or Python), then name the chunk to make it easier to diagnose problems.  After that logical inputs that I use a lot include *eval* (run / don't run the code), *warning* (show/hide warnings), *message* (show/hide other output messages), *cache* (save the output so that the code will not be run next time if nothing has changed).  You can see some of these in the first line of the above code chunk, though these options are only viewable in the Rmd file and will not be viewable in the html rendered version.


### Data Usage

Data needs to be well documented.  Report the data provider, url, and date of retrieval. 


When possible your code should read directly from the original data source rather than a local file.  However, when this is not reasonable include instructions for data acquisition.  In that case a strong option is to set up a Markdown or Jupyter document focusing solely on data acquisition steps and use separate file(s) showcasing data modelling while loading the data from a local (shared?) directory.  One way to stay organized is to prefix the Markdown files and their saved output with a number indicating the order in which files are to be run sequentially.

### Example Data Documentation and Direct Data Download Using Python.


Here we use the monthly **Labour Force Survey Public Use Microfile** from [StatCan](https://www150.statcan.gc.ca/n1/pub/71m0001x/71m0001x2021001-eng.htm). This file directly downloads a zip file from Statistics Canada.  We need to extract both the metadata and csv datafile.

To save time in compiling markdown, use **cache=TRUE** in initializing a code chunk.  In Jupyter notebooks you can cache output by setting the [configurations](https://jupyterbook.org/content/execute.html#caching-the-notebook-execution)


### Loading the data

The Labour Force Survey (LFS) will be downloaded directly from StatCan.  Eventually this code might be put into a loop that will cycle over *year* and *month*, so the code is written without hard coding in these values.  This also makes it easy to recycle these values when saving the cleaned data, so that changes early on in the document propagate through to the output.
Note that in the Rmd code chunks, the argument _cache=TRUE_ is used so that the output at the end of the chunk is saved.  Next time the file is compiled it will load the chunk output rather than run the chunk and re-download the data file.



```{python  getdata, message = FALSE, warning = FALSE}

year  = "2021"
month = "09"
url = "https://www150.statcan.gc.ca/n1/en/pub/71m0001x/2021001/"+year+"-"+month+"-CSV.zip"

# Downloading the file 
req = requests.get(url)

# extracting the zip file contents
zippedfile= zipfile.ZipFile(BytesIO(req.content))

# printing all the contents of the zip file
file_list = zippedfile.printdir()
# extracting all the files
zippedfile.extractall()

# The data of interested in in this file:
lfs_data = pd.read_csv("pub0921.csv")
lfs_data.head()
# datafile is painfully unreadable because of variable names and coding
# See especially, the city CMA or province PROV


# The metadata is located in this csv, 
# note that we need to specify the encoding so that it extracts properly.
# the variable names are not easy to work with so we'll skip the first row
lfs_meta_data = pd.read_csv("LFS_PUMF_EPA_FGMD_variables.csv", header=None,skiprows = 1, encoding = 'ISO-8859-1')
lfs_meta_data.head()


```


### Recoding variables to make them easier to interpret and less likely to introduce errors later in analyses

Locations such as province and CMA are coded numerically, so to avoid errors later on recode them into human interpretable values.  This step makes data and code more readable and easier to verify.  

Start with building a dictionary mapping original harder to interpret encodings into more readable encodings. Creating a data disctionary also improves error checking by keeping track of the mapping from the original to our modified values. This is built by populating the mapping from values in the metadata. At this stage the metadata variables are not well named so we do need to call them by column location.  This is not robust to changes in the metadata formatting and requires openning the metadata, for example using Excel.

From there the data recoding is done automatically.  Some cleaning is performed to make the coded location names easier to handle by removing spaces, accents, and formatting.

```{python   recoding}

# recode province and CMA by extracting coding from the metadata by first finding where in the metadata file the codings are kept:
# build a label dictionary mapping the coded values to the readable values.

#Note that I need to handle the NA values in the metadata for the Boolean search to work properly
province_index = lfs_meta_data[(lfs_meta_data[4].str.contains('prov') & lfs_meta_data[4].notna())].index
cma_index      = lfs_meta_data[(lfs_meta_data[4].str.contains('cma')  & lfs_meta_data[4].notna())].index


# obtain the numerically coded province names
# Obtain the province names from column 12, for the french names use column 13
# rename the columns meaningfully
province_codings = lfs_meta_data.loc[province_index.item()+1:province_index.item()+10,11:12].rename(columns={11:'old_names',12:'new_names'})

# Same for CMAs
cma_codings      = lfs_meta_data.loc[cma_index.item()+1:     cma_index.item()+10,     11:12].rename(columns={11:'old_names',12:'new_names'})

#We can check that the extracted results make sense by uncommenting the province_codings on the next line, but to keep the document clean we will focus on the more challenging CMAs.

# province_codings
cma_codings

# We can also clean up the poor encoding of accent letters or deal with spaces
# This may feel excessive so it's mostly to show how to do this via regex:


# swap "é" with "e" in case this causes problems for my English keyboard later.
cma_codings['new_names'] = cma_codings['new_names'].str.replace("\xe9","e", regex=True)
# punctuation, more encoding, spaces
cma_codings['new_names'] = cma_codings['new_names'].str.replace("\\s|(\x96)","_", regex=True)
cma_codings['new_names'] = cma_codings['new_names'].str.replace("\\(|\\)|\\-","", regex=True)
  
# provinces are simpler since we only need to deal with spaces:
province_codings['new_names'] = province_codings['new_names'].str.replace("\\s", "_", regex=True)


#The dictionary of the location variables is now less likely to introduce errors later on (again to keep the document clean we will just look at the CMAs):

#province_codings
cma_codings

# Now onto recoding provinces and cmas in the LFS data using the dictionaries:
lfs_data['PROV'] = lfs_data['PROV'].replace(province_codings['old_names'].astype('int64').tolist(),  province_codings['new_names'].tolist())

lfs_data['CMA']  = lfs_data['CMA'].replace(      cma_codings['old_names'].astype('int64').tolist(),       cma_codings['new_names'].tolist())
  
# finally look at a few values of the newly recoded variables
lfs_data[['PROV','CMA']].drop_duplicates().head()


```





### Making more descriptive variable names from metadata.

This subsection focuses on ensuring meaningful column names.  The readability could further be improved by recoding but whether or not it is worthwhile to extract more meaningful names from the metadata file depends on the use case, next steps, and collaborators...

Note that in this case recoding variables from their metadata descriptions would lead to non-unique variable names, so here we use a combination of the original non-descriptive names and the metadata descriptions.  While this results in long names, they are very informative and readable.
Again we need to look at the metadata to find the data mapping from coded to readable, informative, and sharable variable names. 



```{python   recodevariablenames}
#lfs_meta_data = pd.read_csv("LFS_PUMF_EPA_FGMD_variables.csv", header=None,skiprows = 1, encoding = 'ISO-8859-1')
# build a mapping from old names to the more descriptive names from the metadata:
new_variable_name_recodings = lfs_meta_data[[4,5]].dropna()
new_variable_name_recodings['old_name'] = new_variable_name_recodings[4].str.upper().str.replace("\\s+", "", regex=True)

new_variable_name_recodings['new_name'] = new_variable_name_recodings['old_name']+"."+new_variable_name_recodings[5].str.upper().str.replace("\\s", "_", regex=True)



# change the variable names to (longer) more descriptive names using the above crafted conventions:
lfs_data = lfs_data[new_variable_name_recodings['old_name']].set_axis( new_variable_name_recodings['new_name'].tolist(), axis = 1)


# this makes the data file much more readable, although longer:
lfs_data.head()


```


## Saving output

Name the output file based on the file name of the Markdown document.  That way the file used to create the data can be clearly found from the datafile.  This also allows datafiles and code used to create those datafiles to group nicely when sorted.  Here, a descriptive name is constructed including the year = `python print(year)` and month = `python print(month)` attributes  defined at the start of the file.  Changes to month and year will then propagate through the document and into the output.  In the Rmd document you can also see the code used for displaying **inline python output**.

```{python   savingoutput}

# construct a file name that is both descriptive and based on the name of the file used to construct and manipulate the data.

#save the code in a general csv format
lfs_data.to_csv("Python_Markdown_obtain_and_prepare_CanSim_files_from_StatCan-"+year+"-"+month+".csv")


```


